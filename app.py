import streamlit as st
import pandas as pd
from datetime import datetime
import json
import os
from typing import Dict, Any
import requests
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í´ë¼ì´ì–¸íŠ¸ import
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    import google.generativeai as genai
    GOOGLE_AVAILABLE = True
except ImportError:
    GOOGLE_AVAILABLE = False

# PDF ì²˜ë¦¬
try:
    import PyPDF2
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="í”„ë¡¬í”„íŠ¸ LLMOps Dashboard", page_icon="ğŸ¤–", layout="wide")

# ëª¨ë¸ë³„ ê°€ê²© ì •ë³´ (USD per 1M tokens)
MODEL_PRICING = {
    "GPT-4o": {"input": 2.50, "output": 10.00},
    "GPT-4o-mini": {"input": 0.15, "output": 0.60},
    "Claude Sonnet 4": {"input": 3.00, "output": 15.00},
    "Claude Sonnet 3.7": {"input": 3.00, "output": 15.00},
    "Perplexity Sonar": {"input": 0.20, "output": 0.20},
    "Perplexity Sonar Pro": {"input": 1.00, "output": 1.00},
    "Gemini 1.5 Flash": {"input": 0.075, "output": 0.30},
    "Gemini 1.5 Pro": {"input": 1.25, "output": 5.00},
    "Gemini 2.0 Flash": {"input": 0.10, "output": 0.40}
}

def calculate_cost(model_name: str, input_tokens: int, output_tokens: int) -> float:
    if model_name not in MODEL_PRICING:
        return 0.0
    pricing = MODEL_PRICING[model_name]
    input_cost = (input_tokens / 1_000_000) * pricing["input"]
    output_cost = (output_tokens / 1_000_000) * pricing["output"]
    return input_cost + output_cost

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'results_history' not in st.session_state:
    st.session_state.results_history = []

def check_api_keys():
    return {
        "OpenAI": bool(os.getenv("OPENAI_API_KEY")),
        "Anthropic": bool(os.getenv("ANTHROPIC_API_KEY")),
        "Gemini": bool(os.getenv("GEMINI_API_KEY")),
        "Perplexity": bool(os.getenv("PERPLEXITY_API_KEY"))
    }

def init_clients():
    clients = {}
    if OPENAI_AVAILABLE and os.getenv("OPENAI_API_KEY"):
        clients['openai'] = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    if ANTHROPIC_AVAILABLE and os.getenv("ANTHROPIC_API_KEY"):
        clients['anthropic'] = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
    if GOOGLE_AVAILABLE and os.getenv("GEMINI_API_KEY"):
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        clients['google'] = genai.GenerativeModel('gemini-1.5-flash')
    return clients

clients = init_clients()

def call_openai(prompt: str, data: str, temperature: float, model_name: str) -> Dict[str, Any]:
    if 'openai' not in clients:
        return {"model": model_name, "response": "OpenAI API í‚¤ ì˜¤ë¥˜", "error": True}
    
    try:
        model_mapping = {"GPT-4o": "gpt-4o", "GPT-4o-mini": "gpt-4o-mini"}
        api_model = model_mapping.get(model_name, "gpt-4o-mini")
        
        if data and data.strip():
            full_prompt = f"ë°ì´í„°: {data}\n\nìš”ì²­: {prompt}"
        else:
            full_prompt = prompt
        
        response = clients['openai'].chat.completions.create(
            model=api_model,
            messages=[{"role": "user", "content": full_prompt}],
            temperature=temperature,
            max_tokens=2000
        )
        
        input_tokens = response.usage.prompt_tokens
        output_tokens = response.usage.completion_tokens
        cost = calculate_cost(model_name, input_tokens, output_tokens)
        
        return {
            "model": model_name,
            "response": response.choices[0].message.content,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "temperature": temperature,
            "tokens_used": input_tokens + output_tokens,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost": cost,
            "error": False
        }
    except Exception as e:
        return {"model": model_name, "response": f"API ì˜¤ë¥˜: {str(e)}", "error": True}

def call_claude(prompt: str, data: str, temperature: float, model_name: str) -> Dict[str, Any]:
    if 'anthropic' not in clients:
        return {"model": model_name, "response": "Anthropic API í‚¤ ì˜¤ë¥˜", "error": True}
    
    try:
        model_mapping = {
            "Claude Sonnet 4": "claude-3-5-sonnet-20241022",
            "Claude Sonnet 3.7": "claude-3-5-sonnet-20241022"
        }
        api_model = model_mapping.get(model_name, "claude-3-5-sonnet-20241022")
        
        if data and data.strip():
            full_prompt = f"ë°ì´í„°: {data}\n\nìš”ì²­: {prompt}"
        else:
            full_prompt = prompt
        
        response = clients['anthropic'].messages.create(
            model=api_model,
            max_tokens=2000,
            temperature=temperature,
            messages=[{"role": "user", "content": full_prompt}]
        )
        
        input_tokens = response.usage.input_tokens
        output_tokens = response.usage.output_tokens
        cost = calculate_cost(model_name, input_tokens, output_tokens)
        
        return {
            "model": model_name,
            "response": response.content[0].text,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "temperature": temperature,
            "tokens_used": input_tokens + output_tokens,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost": cost,
            "error": False
        }
    except Exception as e:
        return {"model": model_name, "response": f"API ì˜¤ë¥˜: {str(e)}", "error": True}

def call_perplexity(prompt: str, data: str, temperature: float, model_name: str) -> Dict[str, Any]:
    api_key = os.getenv("PERPLEXITY_API_KEY")
    if not api_key:
        return {"model": model_name, "response": "Perplexity API í‚¤ ì˜¤ë¥˜", "error": True}
    
    try:
        url = "https://api.perplexity.ai/chat/completions"
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        
        model_mapping = {
            "Perplexity Sonar": "llama-3.1-sonar-small-128k-online",
            "Perplexity Sonar Pro": "llama-3.1-sonar-large-128k-online"
        }
        api_model = model_mapping.get(model_name, "llama-3.1-sonar-small-128k-online")
        
        if data and data.strip():
            full_prompt = f"ë°ì´í„°: {data}\n\nìš”ì²­: {prompt}"
        else:
            full_prompt = prompt
        
        payload = {
            "model": api_model,
            "messages": [{"role": "user", "content": full_prompt}],
            "temperature": temperature,
            "max_tokens": 2000
        }
        
        response = requests.post(url, json=payload, headers=headers, timeout=60)
        response.raise_for_status()
        result = response.json()
        
        input_tokens = result.get('usage', {}).get('prompt_tokens', 0)
        output_tokens = result.get('usage', {}).get('completion_tokens', 0)
        cost = calculate_cost(model_name, input_tokens, output_tokens)
        
        return {
            "model": model_name,
            "response": result['choices'][0]['message']['content'],
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "temperature": temperature,
            "tokens_used": input_tokens + output_tokens,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost": cost,
            "error": False
        }
    except Exception as e:
        return {"model": model_name, "response": f"API ì˜¤ë¥˜: {str(e)}", "error": True}

def call_gemini(prompt: str, data: str, temperature: float, model_name: str) -> Dict[str, Any]:
    if 'google' not in clients:
        return {"model": model_name, "response": "Gemini API í‚¤ ì˜¤ë¥˜", "error": True}
    
    try:
        model_mapping = {
            "Gemini 1.5 Flash": "gemini-1.5-flash",
            "Gemini 1.5 Pro": "gemini-1.5-pro",
            "Gemini 2.0 Flash": "gemini-2.0-flash-exp"
        }
        api_model = model_mapping.get(model_name, "gemini-1.5-flash")
        
        try:
            model_client = genai.GenerativeModel(api_model)
        except:
            model_client = genai.GenerativeModel("gemini-1.5-flash")
        
        if data and data.strip():
            full_prompt = f"ë°ì´í„°: {data}\n\nìš”ì²­: {prompt}"
        else:
            full_prompt = prompt
        
        generation_config = genai.types.GenerationConfig(temperature=temperature, max_output_tokens=2000)
        response = model_client.generate_content(full_prompt, generation_config=generation_config)
        response_text = response.text if hasattr(response, 'text') and response.text else "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"
        
        input_tokens = int(len(full_prompt.split()) * 1.3)
        output_tokens = int(len(response_text.split()) * 1.3)
        cost = calculate_cost(model_name, input_tokens, output_tokens)
        
        return {
            "model": model_name,
            "response": response_text,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "temperature": temperature,
            "tokens_used": input_tokens + output_tokens,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost": cost,
            "error": False
        }
    except Exception as e:
        return {"model": model_name, "response": f"API ì˜¤ë¥˜: {str(e)}", "error": True}

def read_pdf(uploaded_file):
    if not PDF_AVAILABLE:
        return "PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ"
    try:
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        return f"PDF ì½ê¸° ì˜¤ë¥˜: {str(e)}"

# UI
st.title("ğŸš€ í”„ë¡¬í”„íŠ¸ LLMOps Dashboard")

api_status = check_api_keys()

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    # ëª¨ë“œ ì„ íƒ
    execution_mode = st.radio("ì‹¤í–‰ ëª¨ë“œ", ["ë‹¨ì¼ ëª¨ë¸", "ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ"], horizontal=True)
    
    available_models = []
    if api_status["OpenAI"]:
        available_models.extend(["GPT-4o", "GPT-4o-mini"])
    if api_status["Anthropic"]:
        available_models.extend(["Claude Sonnet 4", "Claude Sonnet 3.7"])
    if api_status["Perplexity"]:
        available_models.extend(["Perplexity Sonar", "Perplexity Sonar Pro"])
    if api_status["Gemini"]:
        available_models.extend(["Gemini 1.5 Flash", "Gemini 1.5 Pro", "Gemini 2.0 Flash"])
    
    if not available_models:
        available_models = ["API í‚¤ ì—†ìŒ"]
    
    if execution_mode == "ë‹¨ì¼ ëª¨ë¸":
        model_choice = st.selectbox("ğŸ¤– AI ëª¨ë¸ ì„ íƒ", available_models)
        selected_models = [model_choice] if model_choice != "API í‚¤ ì—†ìŒ" else []
    else:
        selected_models = st.multiselect(
            "ğŸ¤– AI ëª¨ë¸ ì„ íƒ (ìµœëŒ€ 4ê°œ)",
            available_models,
            default=[available_models[0]] if available_models and available_models[0] != "API í‚¤ ì—†ìŒ" else [],
            max_selections=4
        )
    
    temperature = st.slider("ğŸŒ¡ï¸ Temperature", 0.0, 1.0, 0.7, 0.1)
    
    st.markdown("### Made by: KIM JINMAN")
    st.markdown("### Last Update: 2025-06-06")
    st.markdown("### Version: 1.0.1")

# ë©”ì¸ ì˜ì—­
col1, col2 = st.columns([1, 1])

with col1:
    st.header("ğŸ“ ì…ë ¥")
    
    # ë°ì´í„° ì…ë ¥ (ì„ íƒì‚¬í•­)
    input_method = st.radio("ë°ì´í„° ì…ë ¥ ë°©ì‹", ["ì—†ìŒ", "í…ìŠ¤íŠ¸ ì…ë ¥", "íŒŒì¼ ì—…ë¡œë“œ"])
    
    data_input = ""
    
    if input_method == "í…ìŠ¤íŠ¸ ì…ë ¥":
        data_input = st.text_area("ğŸ“„ ë°ì´í„° ì…ë ¥", height=200, placeholder="ë¶„ì„í•  ë°ì´í„°ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒì‚¬í•­)")
    
    elif input_method == "íŒŒì¼ ì—…ë¡œë“œ":
        file_types = ["txt", "csv", "json", "md"]
        if PDF_AVAILABLE:
            file_types.append("pdf")
        
        uploaded_file = st.file_uploader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ", type=file_types)
        
        if uploaded_file is not None:
            try:
                file_extension = uploaded_file.name.lower().split('.')[-1]
                
                if file_extension == 'pdf':
                    data_input = read_pdf(uploaded_file)
                elif file_extension == 'txt':
                    data_input = uploaded_file.getvalue().decode('utf-8')
                elif file_extension == 'csv':
                    df = pd.read_csv(uploaded_file)
                    data_input = df.to_string(index=False)
                elif file_extension == 'json':
                    json_data = json.loads(uploaded_file.getvalue().decode('utf-8'))
                    data_input = json.dumps(json_data, indent=2, ensure_ascii=False)
                elif file_extension == 'md':
                    data_input = uploaded_file.getvalue().decode('utf-8')
                
                if data_input and not data_input.startswith(("PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ", "PDF ì½ê¸° ì˜¤ë¥˜")):
                    st.success(f"âœ… {uploaded_file.name} íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
                    with st.expander("íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°"):
                        st.text_area("", value=data_input[:500] + "..." if len(data_input) > 500 else data_input, height=200, disabled=True)
                else:
                    st.error("íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                st.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
                data_input = ""
    
    # í”„ë¡¬í”„íŠ¸ ì…ë ¥ (í•„ìˆ˜)
    prompt_input = st.text_area("ğŸ’¡ í”„ë¡¬í”„íŠ¸ ì…ë ¥", height=200, placeholder="AIì—ê²Œ ìš”ì²­í•  ì‘ì—…ì„ ì…ë ¥í•˜ì„¸ìš”")
    
    # ì‹¤í–‰ ë²„íŠ¼ - í”„ë¡¬í”„íŠ¸ë§Œ ìˆìœ¼ë©´ ì‹¤í–‰ ê°€ëŠ¥
    can_execute = bool(prompt_input and prompt_input.strip()) and bool(selected_models) and "API í‚¤ ì—†ìŒ" not in selected_models
    
    if st.button("ğŸš€ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰", type="primary", disabled=not can_execute):
        if can_execute:
            with st.spinner(f"AIê°€ ì‘ë‹µì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... ({len(selected_models)}ê°œ ëª¨ë¸)"):
                model_functions = {
                    "GPT-4o": lambda p, d, t: call_openai(p, d, t, "GPT-4o"),
                    "GPT-4o-mini": lambda p, d, t: call_openai(p, d, t, "GPT-4o-mini"),
                    "Claude Sonnet 4": lambda p, d, t: call_claude(p, d, t, "Claude Sonnet 4"),
                    "Claude Sonnet 3.7": lambda p, d, t: call_claude(p, d, t, "Claude Sonnet 3.7"),
                    "Perplexity Sonar": lambda p, d, t: call_perplexity(p, d, t, "Perplexity Sonar"),
                    "Perplexity Sonar Pro": lambda p, d, t: call_perplexity(p, d, t, "Perplexity Sonar Pro"),
                    "Gemini 1.5 Flash": lambda p, d, t: call_gemini(p, d, t, "Gemini 1.5 Flash"),
                    "Gemini 1.5 Pro": lambda p, d, t: call_gemini(p, d, t, "Gemini 1.5 Pro"),
                    "Gemini 2.0 Flash": lambda p, d, t: call_gemini(p, d, t, "Gemini 2.0 Flash")
                }
                
                # ì„ íƒëœ ëª¨ë¸ë“¤ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
                results = []
                for model in selected_models:
                    result = model_functions[model](prompt_input, data_input, temperature)
                    result['prompt'] = prompt_input
                    result['data_preview'] = data_input[:200] + "..." if len(data_input) > 200 else data_input
                    results.append(result)
                    st.session_state.results_history.append(result)
                
                # ë‹¤ì¤‘ ëª¨ë¸ ê²°ê³¼ë¥¼ ì„¸ì…˜ì— ì €ì¥
                if len(selected_models) > 1:
                    st.session_state.multi_results = results
                
                st.rerun()

with col2:
    st.header("ğŸ“Š ê²°ê³¼")
    
    # ë‹¤ì¤‘ ëª¨ë¸ ê²°ê³¼ í‘œì‹œ
    if hasattr(st.session_state, 'multi_results') and st.session_state.multi_results:
        st.subheader("ğŸ”„ ë‹¤ì¤‘ ëª¨ë¸ ë¹„êµ ê²°ê³¼")
        
        # íƒ­ìœ¼ë¡œ ê° ëª¨ë¸ ê²°ê³¼ í‘œì‹œ
        if len(st.session_state.multi_results) > 1:
            tabs = st.tabs([result['model'] for result in st.session_state.multi_results])
            
            for i, (tab, result) in enumerate(zip(tabs, st.session_state.multi_results)):
                with tab:
                    if result.get('error', False):
                        st.error(f"âŒ ì˜¤ë¥˜: {result['response']}")
                    else:
                        st.success(f"âœ… ì‘ë‹µ ì™„ë£Œ")
                        st.text_area("AI ì‘ë‹µ", value=result['response'], height=300, disabled=True, key=f"multi_response_{i}")
                        
                        col_m1, col_m2, col_m3 = st.columns(3)
                        with col_m1:
                            st.metric("í† í°", result.get('tokens_used', 0))
                        with col_m2:
                            st.metric("ì‘ë‹µ ê¸¸ì´", len(result['response']))
                        with col_m3:
                            cost = result.get('cost', 0.0)
                            st.metric("ë¹„ìš©", f"${cost:.6f}")
            
            # ë¹„êµ ìš”ì•½
            st.subheader("ğŸ“ˆ ë¹„êµ ìš”ì•½")
            comparison_data = []
            for result in st.session_state.multi_results:
                comparison_data.append({
                    'ëª¨ë¸': result['model'],
                    'ìƒíƒœ': 'ì„±ê³µ' if not result.get('error', False) else 'ì˜¤ë¥˜',
                    'í† í°': result.get('tokens_used', 0),
                    'ë¹„ìš©': f"${result.get('cost', 0):.6f}",
                    'ì‘ë‹µ ê¸¸ì´': len(result['response'])
                })
            
            df_comparison = pd.DataFrame(comparison_data)
            st.dataframe(df_comparison, use_container_width=True)
    
    # ë‹¨ì¼ ëª¨ë¸ ê²°ê³¼ í‘œì‹œ
    elif st.session_state.results_history:
        latest_result = st.session_state.results_history[-1]
        
        if latest_result.get('error', False):
            st.error(f"âŒ {latest_result['model']}: {latest_result['response']}")
        else:
            st.success(f"âœ… {latest_result['model']} ì‘ë‹µ ì™„ë£Œ")
            st.text_area("AI ì‘ë‹µ", value=latest_result['response'], height=400, disabled=True)
            
            col_metric1, col_metric2, col_metric3 = st.columns(3)
            with col_metric1:
                st.metric("í† í°", latest_result.get('tokens_used', 0))
            with col_metric2:
                st.metric("ì‘ë‹µ ê¸¸ì´", len(latest_result['response']))
            with col_metric3:
                cost = latest_result.get('cost', 0.0)
                st.metric("ë¹„ìš©", f"${cost:.6f}")

# ê¸°ë¡ ì„¹ì…˜
if st.session_state.results_history:
    st.markdown("---")
    st.header("ğŸ“š ê¸°ë¡")
    
    col_clear, col_download = st.columns([1, 1])
    with col_clear:
        if st.button("ğŸ—‘ï¸ ê¸°ë¡ ì´ˆê¸°í™”"):
            st.session_state.results_history = []
            st.rerun()
    
    with col_download:
        export_data = []
        for record in st.session_state.results_history:
            export_data.append({
                'ì‹œê°„': record.get('timestamp', ''),
                'ëª¨ë¸': record.get('model', ''),
                'í”„ë¡¬í”„íŠ¸': record.get('prompt', ''),
                'ì‘ë‹µ': record.get('response', ''),
                'í† í°': record.get('tokens_used', 0),
                'ë¹„ìš©': record.get('cost', 0),
                'ìƒíƒœ': 'ì˜¤ë¥˜' if record.get('error', False) else 'ì„±ê³µ'
            })
        
        df_export = pd.DataFrame(export_data)
        
        # TXT íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        txt_data = f"""LLMOps Dashboard ê¸°ë¡
ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
{'='*50}

"""
        for i, record in enumerate(st.session_state.results_history, 1):
            txt_data += f"[ê¸°ë¡ {i}]\n"
            txt_data += f"ì‹œê°„: {record.get('timestamp', '')}\n"
            txt_data += f"ëª¨ë¸: {record.get('model', '')}\n"
            txt_data += f"í”„ë¡¬í”„íŠ¸: {record.get('prompt', '')}\n"
            txt_data += f"ì‘ë‹µ: {record.get('response', '')}\n"
            txt_data += f"í† í°: {record.get('tokens_used', 0)}\n"
            txt_data += f"ë¹„ìš©: ${record.get('cost', 0):.6f}\n"
            txt_data += f"ìƒíƒœ: {'ì˜¤ë¥˜' if record.get('error', False) else 'ì„±ê³µ'}\n"
            txt_data += "-" * 50 + "\n\n"
        
        st.download_button(
            "ğŸ“„ TXT ë‹¤ìš´ë¡œë“œ",
            data=txt_data,
            file_name=f"llmops_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
            mime="text/plain"
        )
    
    # ì›¹ì—ì„œ ë³´ê¸°
    with st.expander("ğŸŒ ì›¹ì—ì„œ ë³´ê¸° (ë³µì‚¬ìš©)"):
        st.dataframe(df_export, use_container_width=True)
        st.caption("ìœ„ í‘œë¥¼ ì„ íƒí•˜ì—¬ ë³µì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    
    # ìµœê·¼ 5ê°œ ê¸°ë¡ í‘œì‹œ
    recent_history = st.session_state.results_history[-5:][::-1]
    
    for i, record in enumerate(recent_history):
        status_icon = "âŒ" if record.get('error', False) else "âœ…"
        with st.expander(f"{status_icon} {record['timestamp']} - {record['model']}", expanded=(i==0)):
            st.text(f"í”„ë¡¬í”„íŠ¸: {record.get('prompt', 'N/A')}")
            if record.get('error', False):
                st.error(record['response'])
            else:
                st.text_area("ì‘ë‹µ", value=record['response'], height=150, key=f"history_{i}", disabled=True)
                st.caption(f"í† í°: {record.get('tokens_used', 0)} | ë¹„ìš©: ${record.get('cost', 0):.6f}")
import streamlit as st
import pandas as pd
from datetime import datetime
import json
import os
from typing import Dict, Any, Optional
import requests
import time
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í´ë¼ì´ì–¸íŠ¸ import
try:
    import openai
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    import google.generativeai as genai
    GOOGLE_AVAILABLE = True
except ImportError:
    GOOGLE_AVAILABLE = False

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="í”„ë¡¬í”„íŠ¸ LLMOps Dashboard",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    
    .result-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 20px;
        border-radius: 15px;
        color: white;
        margin: 20px 0;
    }
    
    .metric-card {
        background: white;
        padding: 15px;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        margin: 10px 0;
    }
    
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 5px;
        padding: 15px;
        margin: 10px 0;
    }
    
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        border-radius: 5px;
        padding: 15px;
        margin: 10px 0;
    }
    
    .error-box {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        border-radius: 5px;
        padding: 15px;
        margin: 10px 0;
    }
</style>
""", unsafe_allow_html=True)

# ëª¨ë¸ë³„ ê°€ê²© ì •ë³´ (USD per 1M tokens)
MODEL_PRICING = {
    # OpenAI
    "GPT-4o": {"input": 2.50, "output": 10.00},
    "GPT-4o-mini": {"input": 0.15, "output": 0.60},
    
    # Claude (Anthropic)
    "Claude Sonnet 4": {"input": 3.00, "output": 15.00},
    "Claude Sonnet 3.5": {"input": 3.00, "output": 15.00},
    "Claude Haiku 3.5": {"input": 0.25, "output": 1.25},
    
    # Perplexity
    "Perplexity Sonar": {"input": 0.20, "output": 0.20},
    "Perplexity Sonar Pro": {"input": 1.00, "output": 1.00},
    
    # Gemini
    "Gemini 1.5 Flash": {"input": 0.075, "output": 0.30},
    "Gemini 1.5 Pro": {"input": 1.25, "output": 5.00},
    "Gemini 2.0 Flash": {"input": 0.10, "output": 0.40}
}

def truncate_text(text: str, max_tokens: int = 100000) -> str:
    """í…ìŠ¤íŠ¸ë¥¼ í† í° í•œë„ì— ë§ì¶° ìë¥´ê¸° (ëŒ€ëµì  ê³„ì‚°)"""
    # ëŒ€ëµì ìœ¼ë¡œ 1í† í° = 4ë¬¸ìë¡œ ê³„ì‚°
    max_chars = max_tokens * 4
    if len(text) <= max_chars:
        return text
    
    # í…ìŠ¤íŠ¸ë¥¼ ìë¥´ë˜, ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ë ¤ê³  ì‹œë„
    truncated = text[:max_chars]
    
    # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ê¹Œì§€ë§Œ í¬í•¨
    last_period = truncated.rfind('.')
    last_newline = truncated.rfind('\n')
    
    cut_point = max(last_period, last_newline)
    if cut_point > max_chars * 0.8:  # 80% ì´ìƒì´ë©´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¦„
        truncated = truncated[:cut_point + 1]
    
    return truncated + "\n\n[... í…ìŠ¤íŠ¸ê°€ í† í° í•œë„ë¡œ ì¸í•´ ì˜ë ¸ìŠµë‹ˆë‹¤ ...]"

def calculate_cost(model_name: str, input_tokens: int, output_tokens: int) -> float:
    """ëª¨ë¸ë³„ ë¹„ìš© ê³„ì‚°"""
    if model_name not in MODEL_PRICING:
        return 0.0
    
    pricing = MODEL_PRICING[model_name]
    input_cost = (input_tokens / 1_000_000) * pricing["input"]
    output_cost = (output_tokens / 1_000_000) * pricing["output"]
    return input_cost + output_cost

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'results_history' not in st.session_state:
    st.session_state.results_history = []

if 'current_result' not in st.session_state:
    st.session_state.current_result = None

# API í‚¤ í™•ì¸ ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
def check_api_keys():
    """API í‚¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    keys_status = {
        "OpenAI": bool(os.getenv("OPENAI_API_KEY")),
        "Anthropic": bool(os.getenv("ANTHROPIC_API_KEY")),
        "Gemini": bool(os.getenv("GEMINI_API_KEY")),
        "Perplexity": bool(os.getenv("PERPLEXITY_API_KEY"))
    }
    return keys_status

def init_clients():
    """API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    clients = {}
    
    # OpenAI í´ë¼ì´ì–¸íŠ¸
    if OPENAI_AVAILABLE and os.getenv("OPENAI_API_KEY"):
        try:
            clients['openai'] = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        except Exception as e:
            st.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    # Anthropic í´ë¼ì´ì–¸íŠ¸
    if ANTHROPIC_AVAILABLE and os.getenv("ANTHROPIC_API_KEY"):
        try:
            clients['anthropic'] = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        except Exception as e:
            st.error(f"Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    # Google í´ë¼ì´ì–¸íŠ¸
    if GOOGLE_AVAILABLE and os.getenv("GEMINI_API_KEY"):
        try:
            genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
            clients['google'] = genai.GenerativeModel('gemini-1.5-flash')  # ê¸°ë³¸ ëª¨ë¸ ë³€ê²½
        except Exception as e:
            st.error(f"Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    return clients

# API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
clients = init_clients()

# ì‹¤ì œ API í˜¸ì¶œ í•¨ìˆ˜ë“¤
def call_openai(prompt: str, data: str, temperature: float, model_name: str) -> Dict[str, Any]:
    """OpenAI GPT API ì‹¤ì œ í˜¸ì¶œ"""
    if 'openai' not in clients:
        return {
            "model": model_name,
            "response": "âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "temperature": temperature,
            "tokens_used": 0,
            "input_tokens": 0,
            "output_tokens": 0,
            "cost": 0.0,
            "error": True
        }
    
    try:
        # ëª¨ë¸ëª… ë§¤í•‘ (GPT-4.1 ì œê±°)
        model_mapping = {
            "GPT-4o": "gpt-4o",                # GPT-4o ëª¨ë¸
            "GPT-4o-mini": "gpt-4o-mini"       # GPT-4o-mini ëª¨ë¸
        }
        
        api_model = model_mapping.get(model_name, "gpt-4o-mini")
        
        # ëª¨ë¸ë³„ í† í° í•œë„ ì„¤ì •
        model_token_limits = {
            "gpt-4o": 128000,          # 128k í† í° ì»¨í…ìŠ¤íŠ¸  
            "gpt-4o-mini": 128000      # 128k í† í° ì»¨í…ìŠ¤íŠ¸
        }
        
        max_input_tokens = model_token_limits.get(api_model, 128000) - 4096  # ì¶œë ¥ìš© í† í° ì˜ˆì•½
        
        # ë°ì´í„° í¬ê¸° í™•ì¸ ë° ìë¥´ê¸°
        data_truncated = truncate_text(data, max_input_tokens // 2)  # ì ˆë°˜ì€ ë°ì´í„°, ì ˆë°˜ì€ í”„ë¡¬í”„íŠ¸+ì‘ë‹µìš©
        prompt_truncated = truncate_text(prompt, 4000)  # í”„ë¡¬í”„íŠ¸ëŠ” 4000í† í°ìœ¼ë¡œ ì œí•œ
        
        # í”„ë¡¬í”„íŠ¸ì™€ ë°ì´í„° ê²°í•©
        full_prompt = f"ë°ì´í„°: {data_truncated}\n\nìš”ì²­: {prompt_truncated}"
        
        # ì…ë ¥ í† í° ìˆ˜ ëŒ€ëµ ê³„ì‚°
        estimated_input_tokens = len(full_prompt) // 4
        
        if estimated_input_tokens > max_input_tokens:
            return {
                "model": model_name,
                "response": f"âŒ ì…ë ¥ ë°ì´í„°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ì˜ˆìƒ í† í°: {estimated_input_tokens:,}, í•œë„: {max_input_tokens:,}. ë” ì‘ì€ íŒŒì¼ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ë°ì´í„°ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.",
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "temperature": temperature,
                "tokens_used": 0,
                "input_tokens": 0,
                "output_tokens": 0,
                "cost": 0.0,
                "error": True
            }
        
        response = clients['openai'].chat.completions.create(
            model=api_model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                {"role": "user", "content": full_prompt}
            ],
            temperature=temperature,
            max_tokens=min(int(os.getenv("DEFAULT_MAX_TOKENS", 2000)), 4000),  # ì¶œë ¥ í† í° ì œí•œ
            timeout=60  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        )
        
        input_tokens = response.usage.prompt_tokens
        output_tokens = response.usage.completion_tokens
        total_tokens = response.usage.total_tokens
        cost = calculate_cost(model_name, input_tokens, output_tokens)
        
        response_text = response.choices[0].message.content
        
        # ë°ì´í„°ê°€ ì˜ë ¸ìœ¼ë©´ ì•Œë¦¼ ì¶”ê°€
        if len(data) != len(data_truncated):
            response_text = f"âš ï¸ ì…ë ¥ ë°ì´í„°ê°€ í† í° í•œë„ë¡œ ì¸í•´ ì˜ë ¸ìŠµë‹ˆë‹¤.\n\n{response_text}"
        
        return {
            "model": model_name,
            "response": response_text,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "temperature": temperature,
            "tokens_used": total_tokens,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost": cost,
            "error": False
        }
        
    except Exception as e:
        error_msg = str(e)
        if "timeout" in error_msg.lower():
            error_msg = f"â±ï¸ ìš”ì²­ ì‹œê°„ ì´ˆê³¼: {error_msg}\n\nğŸ’¡ í•´ê²° ë°©ë²•:\n- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„\n- ì…ë ¥ ë°ì´í„° í¬ê¸° ì¤„ì´ê¸°\n- ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©í•´ë³´ê¸°"
        
        return {
            "model": model_name,
            "response": f"âŒ API í˜¸ì¶œ ì˜¤ë¥˜: {error_msg}",
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "temperature": temperature,
            "tokens_used": 0,
            "input_tokens": 0,
            "output_tokens": 0,
            "cost": 0.0,
            "error": True
        }

def call_claude(prompt: str, data: str, temperature: float, model_name: str) -> Dict[str, Any]:
    """Claude API ì‹¤ì œ í˜¸ì¶œ"""
    if 'anthropic' not in clients:
        return {
            "model": model_name,
            "response": "âŒ Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ anthropic íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "temperature": temperature,
            "tokens_used": 0,
            "input_tokens": 0,
            "output_tokens": 0,
            "cost": 0.0,
            "error": True
        }
    
    try:
        # ëª¨ë¸ëª… ë§¤í•‘
        model_mapping = {
            "Claude Sonnet 4": "claude-3-5-sonnet-20241022",
            "Claude Sonnet 3.5": "claude-3-5-sonnet-20240620",
            "Claude Haiku 3.5": "claude-3-5-haiku-20241022"
        }
        
        api_model = model_mapping.get(model_name, "claude-3-5-sonnet-20241022")
        
        full_prompt = f"ë°ì´í„°: {data}\n\nìš”ì²­: {prompt}"
        
        response = clients['anthropic'].messages.create(
            model=api_model,
            max_tokens=int(os.getenv("DEFAULT_MAX_TOKENS", 2000)),
            temperature=temperature,
            messages=[
                {"role": "user", "content": full_prompt}
            ]
        )
        
        input_tokens = response.usage.input_tokens
        output_tokens = response.usage.output_tokens
        total_tokens = input_tokens + output_tokens
        cost = calculate_cost(model_name, input_tokens, output_tokens)
        
        return {
            "model": model_name,
            "response": response.content[0].text,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "temperature": temperature,
            "tokens_used": total_tokens,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost": cost,
            "error": False
        }
        
    except Exception as e:
        return {
            "model": model_name,
            "response": f"âŒ API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}",
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "temperature": temperature,
            "tokens_used": 0,
            "input_tokens": 0,
            "output_tokens": 0,
            "cost": 0.0,
            "error": True
        }

def call_perplexity(prompt: str, data: str, temperature: float, model_name: str) -> Dict[str, Any]:
    """Perplexity API ì‹¤ì œ í˜¸ì¶œ"""
    api_key = os.getenv("PERPLEXITY_API_KEY")
    if not api_key:
        return {
            "model": model_name,
            "response": "âŒ Perplexity API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "temperature": temperature,
            "tokens_used": 0,
            "input_tokens": 0,
            "output_tokens": 0,
            "cost": 0.0,
            "error": True
        }
    
    try:
        url = "https://api.perplexity.ai/chat/completions"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # ëª¨ë¸ëª… ë§¤í•‘
        model_mapping = {
            "Perplexity Sonar": "llama-3.1-sonar-small-128k-online",
            "Perplexity Sonar Pro": "llama-3.1-sonar-large-128k-online"
        }
        
        api_model = model_mapping.get(model_name, "llama-3.1-sonar-small-128k-online")
        
        full_prompt = f"ë°ì´í„°: {data}\n\nìš”ì²­: {prompt}"
        
        payload = {
            "model": api_model,
            "messages": [
                {"role": "user", "content": full_prompt}
            ],
            "temperature": temperature,
            "max_tokens": int(os.getenv("DEFAULT_MAX_TOKENS", 2000))
        }
        
        response = requests.post(url, json=payload, headers=headers, timeout=60)  # 60ì´ˆë¡œ ì¦ê°€
        response.raise_for_status()
        
        result = response.json()
        
        input_tokens = result.get('usage', {}).get('prompt_tokens', 0)
        output_tokens = result.get('usage', {}).get('completion_tokens', 0)
        total_tokens = result.get('usage', {}).get('total_tokens', input_tokens + output_tokens)
        cost = calculate_cost(model_name, input_tokens, output_tokens)
        
        return {
            "model": model_name,
            "response": result['choices'][0]['message']['content'],
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "temperature": temperature,
            "tokens_used": total_tokens,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "cost": cost,
            "error": False
        }
        
    except Exception as e:
        return {
            "model": model_name,
            "response": f"âŒ API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}",
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "temperature": temperature,
            "tokens_used": 0,
            "input_tokens": 0,
            "output_tokens": 0,
            "cost": 0.0,
            "error": True
        }

def call_gemini(prompt: str, data: str, temperature: float, model_name: str) -> Dict[str, Any]:
    """Gemini API ì‹¤ì œ í˜¸ì¶œ"""
    if 'google' not in clients:
        return {
            "model": model_name,
            "response": "âŒ Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ google-generativeai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "temperature": temperature,
            "tokens_used": 0,
            "input_tokens": 0,
            "output_tokens": 0,
            "cost": 0.0,
            "error": True
        }
    
    try:
        # ëª¨ë¸ëª… ë§¤í•‘
        model_mapping = {
            "Gemini 1.5 Flash": "gemini-1.5-flash",
            "Gemini 1.5 Pro": "gemini-1.5-pro",
            "Gemini 2.0 Flash": "gemini-2.0-flash-exp"
        }
        
        api_model = model_mapping.get(model_name, "gemini-1.5-flash")
        
        # ìƒˆë¡œìš´ ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        try:
            model_client = genai.GenerativeModel(api_model)
        except Exception:
            # ëª¨ë¸ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
            model_client = genai.GenerativeModel("gemini-1.5-flash")
        
        full_prompt = f"ë°ì´í„°: {data}\n\nìš”ì²­: {prompt}"
        
        # Generation config ì„¤ì •
        generation_config = genai.types.GenerationConfig(
            temperature=temperature,
            max_output_tokens=int(os.getenv("DEFAULT_MAX_TOKENS", 2000))
        )
        
        response = model_client.generate_content(
            full_prompt,
            generation_config=generation_config
        )
        
        # ì‘ë‹µ í…ìŠ¤íŠ¸ í™•ì¸
        response_text = ""
        if response and hasattr(response, 'text') and response.text:
            response_text = response.text
        elif response and hasattr(response, 'candidates') and response.candidates:
            # candidatesì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
            try:
                response_text = response.candidates[0].content.parts[0].text
            except (IndexError, AttributeError):
                response_text = "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            response_text = "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚° (ê·¼ì‚¬ì¹˜)
        input_tokens = len(full_prompt.split()) * 1.3  # ë‹¨ì–´ ìˆ˜ Ã— 1.3 (ê·¼ì‚¬ì¹˜)
        output_tokens = len(response_text.split()) * 1.3 if response_text else 0
        total_tokens = int(input_tokens + output_tokens)
        cost = calculate_cost(model_name, int(input_tokens), int(output_tokens))
        
        return {
            "model": model_name,
            "response": response_text,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "temperature": temperature,
            "tokens_used": total_tokens,
            "input_tokens": int(input_tokens),
            "output_tokens": int(output_tokens),
            "cost": cost,
            "error": False
        }
        
    except Exception as e:
        return {
            "model": model_name,
            "response": f"âŒ API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}",
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "temperature": temperature,
            "tokens_used": 0,
            "input_tokens": 0,
            "output_tokens": 0,
            "cost": 0.0,
            "error": True
        }

# ë©”ì¸ í—¤ë”
st.markdown('<h1 class="main-header">ğŸš€ í”„ë¡¬í”„íŠ¸ LLMOps Dashboard</h1>', unsafe_allow_html=True)

# API í‚¤ ìƒíƒœ í™•ì¸
api_status = check_api_keys()

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ëª¨ë¸ ì„¤ì •")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë§Œ ì„ íƒì§€ì— í‘œì‹œ (GPT-4.1 ì œê±°)
    available_models = []
    if api_status["OpenAI"]:
        available_models.extend(["GPT-4o", "GPT-4o-mini"])
    if api_status["Anthropic"]:
        available_models.extend(["Claude Sonnet 4", "Claude Sonnet 3.5", "Claude Haiku 3.5"])
    if api_status["Perplexity"]:
        available_models.extend(["Perplexity Sonar", "Perplexity Sonar Pro"])
    if api_status["Gemini"]:
        available_models.extend(["Gemini 1.5 Flash", "Gemini 1.5 Pro", "Gemini 2.0 Flash"])
    
    if not available_models:
        available_models = ["ë°ëª¨ ëª¨ë“œ"]
        st.error("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    # ëª¨ë¸ ì„ íƒ
    model_choice = st.selectbox(
        "ğŸ¤– AI ëª¨ë¸ ì„ íƒ",
        available_models,
        help="ì‚¬ìš©í•  AI ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”"
    )
    
    # í…œí¼ì²˜ ì¡°ì •
    temperature = st.slider(
        "ğŸŒ¡ï¸ Temperature (ì°½ì˜ì„±)",
        min_value=0.0,
        max_value=1.0,
        value=float(os.getenv("DEFAULT_TEMPERATURE", 0.7)),
        step=0.1,
        help="ë‚®ì„ìˆ˜ë¡ ì¼ê´€ëœ ë‹µë³€, ë†’ì„ìˆ˜ë¡ ì°½ì˜ì  ë‹µë³€"
    )
    
    # í˜„ì¬ ì„¤ì • í‘œì‹œ
    st.markdown("---")
    st.markdown("### ğŸ“Š í˜„ì¬ ì„¤ì •")
    st.info(f"**ëª¨ë¸**: {model_choice}\n**ì°½ì˜ì„±**: {temperature}")
    
    # í†µê³„
    if st.session_state.results_history:
        st.markdown("### ğŸ“ˆ ì„¸ì…˜ í†µê³„")
        total_queries = len(st.session_state.results_history)
        st.metric("ì´ ì§ˆì˜ ìˆ˜", total_queries)
        
        models_used = [r['model'] for r in st.session_state.results_history]
        most_used = max(set(models_used), key=models_used.count)
        st.metric("ìµœë‹¤ ì‚¬ìš© ëª¨ë¸", most_used)
        
        # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„
        successful_queries = len([r for r in st.session_state.results_history if not r.get('error', False)])
        success_rate = (successful_queries / total_queries * 100) if total_queries > 0 else 0
        st.metric("ì„±ê³µë¥ ", f"{success_rate:.1f}%")

# ë©”ì¸ ì½˜í…ì¸  ì˜ì—­
col1, col2 = st.columns([1, 1])

with col1:
    st.header("ğŸ“ ì…ë ¥ ì„¹ì…˜")
    
    # ë°ì´í„° ì…ë ¥ ë°©ì‹ ì„ íƒ
    input_method = st.radio(
        "ë°ì´í„° ì…ë ¥ ë°©ì‹",
        ["í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥", "íŒŒì¼ ì—…ë¡œë“œ"],
        horizontal=True
    )
    
    data_input = ""
    
    if input_method == "í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥":
        data_input = st.text_area(
            "ğŸ“„ ë°ì´í„° ì…ë ¥",
            height=300,  # 150 â†’ 300ìœ¼ë¡œ ì¦ê°€
            placeholder="ë¶„ì„í•˜ê³  ì‹¶ì€ ë°ì´í„°ë‚˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."
        )
    else:
        allowed_types = os.getenv("ALLOWED_FILE_TYPES", "txt,csv,json,md").split(",")
        uploaded_file = st.file_uploader(
            "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ",
            type=allowed_types,
            help=f"ì§€ì› íŒŒì¼ í˜•ì‹: {', '.join(allowed_types)}"
        )
        
        if uploaded_file is not None:
            try:
                # íŒŒì¼ í¬ê¸° í™•ì¸
                max_size = int(os.getenv("MAX_FILE_SIZE_MB", 10)) * 1024 * 1024
                if uploaded_file.size > max_size:
                    st.error(f"âŒ íŒŒì¼ í¬ê¸°ê°€ {os.getenv('MAX_FILE_SIZE_MB', 10)}MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.")
                    data_input = ""
                else:
                    # íŒŒì¼ í¬ì¸í„°ë¥¼ ì²˜ìŒìœ¼ë¡œ ë˜ëŒë¦¬ê¸°
                    uploaded_file.seek(0)
                    
                    # íŒŒì¼ ì½ê¸° ì‹œë„
                    file_content = None
                    
                    # íŒŒì¼ í™•ì¥ì ê¸°ë°˜ ì²˜ë¦¬
                    file_extension = uploaded_file.name.lower().split('.')[-1]
                    
                    if file_extension == 'txt' or uploaded_file.type == "text/plain":
                        try:
                            bytes_data = uploaded_file.getvalue()
                            file_content = bytes_data.decode('utf-8')
                        except UnicodeDecodeError:
                            try:
                                file_content = bytes_data.decode('utf-8', errors='replace')
                            except:
                                file_content = bytes_data.decode('latin-1', errors='ignore')
                                
                    elif file_extension == 'csv' or uploaded_file.type == "text/csv":
                        try:
                            # CSV íŒŒì¼ì„ DataFrameìœ¼ë¡œ ì½ê¸°
                            df = pd.read_csv(uploaded_file, encoding='utf-8')
                            file_content = df.to_string(index=False)
                        except UnicodeDecodeError:
                            uploaded_file.seek(0)
                            df = pd.read_csv(uploaded_file, encoding='latin-1')
                            file_content = df.to_string(index=False)
                        except Exception:
                            uploaded_file.seek(0)
                            try:
                                df = pd.read_csv(uploaded_file, encoding='cp949')  # í•œê¸€ ì¸ì½”ë”©
                                file_content = df.to_string(index=False)
                            except:
                                st.error("âŒ CSV íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                                file_content = None
                                
                    elif file_extension == 'json' or uploaded_file.type == "application/json":
                        try:
                            bytes_data = uploaded_file.getvalue()
                            json_str = bytes_data.decode('utf-8')
                            json_data = json.loads(json_str)
                            file_content = json.dumps(json_data, indent=2, ensure_ascii=False)
                        except UnicodeDecodeError:
                            try:
                                json_str = bytes_data.decode('utf-8', errors='replace')
                                json_data = json.loads(json_str)
                                file_content = json.dumps(json_data, indent=2, ensure_ascii=False)
                            except:
                                st.error("âŒ JSON íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                file_content = None
                        except json.JSONDecodeError:
                            st.error("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ JSON í˜•ì‹ì…ë‹ˆë‹¤.")
                            file_content = None
                            
                    elif file_extension == 'md':
                        try:
                            bytes_data = uploaded_file.getvalue()
                            file_content = bytes_data.decode('utf-8')
                        except UnicodeDecodeError:
                            try:
                                file_content = bytes_data.decode('utf-8', errors='replace')
                            except:
                                file_content = bytes_data.decode('latin-1', errors='ignore')
                                
                    else:
                        # ê¸°ë³¸ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¡œ ì½ê¸° ì‹œë„
                        try:
                            bytes_data = uploaded_file.getvalue()
                            file_content = bytes_data.decode('utf-8')
                        except UnicodeDecodeError:
                            try:
                                file_content = bytes_data.decode('utf-8', errors='replace')
                            except:
                                file_content = bytes_data.decode('latin-1', errors='ignore')
                    
                    # ê²°ê³¼ ì²˜ë¦¬
                    if file_content and file_content.strip():
                        data_input = file_content
                        st.success(f"âœ… {uploaded_file.name} íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.info(f"ğŸ“Š íŒŒì¼ í¬ê¸°: {len(data_input):,} ë¬¸ì")
                        
                        # íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                        with st.expander("ğŸ“„ íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°"):
                            preview_text = data_input[:1000] + ("..." if len(data_input) > 1000 else "")
                            st.text_area(
                                "íŒŒì¼ ë‚´ìš©",
                                value=preview_text,
                                height=300,  # 200 â†’ 300ìœ¼ë¡œ ì¦ê°€
                                disabled=True,
                                key="file_preview"
                            )
                            
                        # í† í° ì‚¬ìš©ëŸ‰ ê²½ê³ 
                        estimated_tokens = len(data_input) // 4
                        if estimated_tokens > 100000:
                            st.warning(f"âš ï¸ í° íŒŒì¼ì…ë‹ˆë‹¤! ì˜ˆìƒ í† í°: {estimated_tokens:,}ê°œ. API í•œë„ ì´ˆê³¼ ì‹œ ìë™ìœ¼ë¡œ ì˜ë¦½ë‹ˆë‹¤.")
                        elif estimated_tokens > 50000:
                            st.info(f"ğŸ“Š ì¤‘ê°„ í¬ê¸° íŒŒì¼ì…ë‹ˆë‹¤. ì˜ˆìƒ í† í°: {estimated_tokens:,}ê°œ")
                    else:
                        st.error("âŒ íŒŒì¼ ë‚´ìš©ì´ ë¹„ì–´ìˆê±°ë‚˜ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        data_input = ""
                    
            except Exception as e:
                st.error(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")
                st.error(f"ğŸ” ë””ë²„ê·¸ ì •ë³´: íŒŒì¼ëª…={uploaded_file.name}, í¬ê¸°={uploaded_file.size}, íƒ€ì…={uploaded_file.type}")
                data_input = ""
    
    # í”„ë¡¬í”„íŠ¸ ì…ë ¥
    prompt_input = st.text_area(
        "ğŸ’¡ í”„ë¡¬í”„íŠ¸ ì…ë ¥",
        height=400,  
        placeholder="AIì—ê²Œ ìš”ì²­í•  ì‘ì—…ì„ êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”..."
    )
    
    # ì‹¤í–‰ ë²„íŠ¼
    can_execute = (
        bool(data_input and data_input.strip()) and 
        bool(prompt_input and prompt_input.strip()) and 
        model_choice != "ë°ëª¨ ëª¨ë“œ" and
        any(api_status.values())
    )
    
    # ë””ë²„ê·¸ ì •ë³´ (ê°œë°œìš© - ë‚˜ì¤‘ì— ì œê±° ê°€ëŠ¥)
    if os.getenv("DEBUG_MODE", "False").lower() == "true":
        st.write(f"Debug - data_input ê¸¸ì´: {len(data_input) if data_input else 0}")
        st.write(f"Debug - prompt_input ê¸¸ì´: {len(prompt_input) if prompt_input else 0}")
        st.write(f"Debug - model_choice: {model_choice}")
        st.write(f"Debug - can_execute: {can_execute}")
    
    execute_button = st.button(
        "ğŸš€ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰",
        type="primary",
        use_container_width=True,
        disabled=not can_execute
    )
    
    # ë²„íŠ¼ì´ ë¹„í™œì„±í™”ëœ ì´ìœ  í‘œì‹œ
    if not can_execute:
        if not data_input or not data_input.strip():
            st.warning("âš ï¸ ë°ì´í„°ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        elif not prompt_input or not prompt_input.strip():
            st.warning("âš ï¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        elif model_choice == "ë°ëª¨ ëª¨ë“œ":
            st.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        elif not any(api_status.values()):
            st.warning("âš ï¸ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")

with col2:
    st.header("ğŸ“Š ê²°ê³¼ ì„¹ì…˜")
    
    if execute_button and can_execute:
        with st.spinner("AIê°€ ì‘ë‹µì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            # ëª¨ë¸ë³„ API í˜¸ì¶œ (GPT-4.1 ì œê±°)
            model_functions = {
                "GPT-4o": lambda p, d, t: call_openai(p, d, t, "GPT-4o"),
                "GPT-4o-mini": lambda p, d, t: call_openai(p, d, t, "GPT-4o-mini"),
                "Claude Sonnet 4": lambda p, d, t: call_claude(p, d, t, "Claude Sonnet 4"),
                "Claude Sonnet 3.5": lambda p, d, t: call_claude(p, d, t, "Claude Sonnet 3.5"),
                "Claude Haiku 3.5": lambda p, d, t: call_claude(p, d, t, "Claude Haiku 3.5"),
                "Perplexity Sonar": lambda p, d, t: call_perplexity(p, d, t, "Perplexity Sonar"),
                "Perplexity Sonar Pro": lambda p, d, t: call_perplexity(p, d, t, "Perplexity Sonar Pro"),
                "Gemini 1.5 Flash": lambda p, d, t: call_gemini(p, d, t, "Gemini 1.5 Flash"),
                "Gemini 1.5 Pro": lambda p, d, t: call_gemini(p, d, t, "Gemini 1.5 Pro"),
                "Gemini 2.0 Flash": lambda p, d, t: call_gemini(p, d, t, "Gemini 2.0 Flash")
            }
            
            # ì‹¤ì œ API í˜¸ì¶œ
            result = model_functions[model_choice](prompt_input, data_input, temperature)
            result['prompt'] = prompt_input
            result['data_preview'] = data_input[:200] + "..." if len(data_input) > 200 else data_input
            
            # í˜„ì¬ ê²°ê³¼ ì €ì¥
            st.session_state.current_result = result
            
            # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            st.session_state.results_history.append(result)
    
    # í˜„ì¬ ê²°ê³¼ í‘œì‹œ
    if st.session_state.current_result:
        result = st.session_state.current_result
        
        # ì—ëŸ¬ ì—¬ë¶€ì— ë”°ë¥¸ ìŠ¤íƒ€ì¼ë§
        if result.get('error', False):
            container_class = "error-box"
        else:
            container_class = "result-container"
        
        # ê²°ê³¼ ì»¨í…Œì´ë„ˆ
        if result.get('error', False):
            st.markdown(f"""
            <div class="{container_class}">
                <h3>âŒ ì˜¤ë¥˜ ë°œìƒ</h3>
                <p><strong>ëª¨ë¸:</strong> {result['model']}</p>
                <p><strong>ì‹œê°„:</strong> {result['timestamp']}</p>
                <p><strong>ì˜¨ë„:</strong> {result['temperature']}</p>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
            <div class="{container_class}">
                <h3>ğŸ¯ ìµœì‹  ê²°ê³¼</h3>
                <p><strong>ëª¨ë¸:</strong> {result['model']}</p>
                <p><strong>ì‹œê°„:</strong> {result['timestamp']}</p>
                <p><strong>ì˜¨ë„:</strong> {result['temperature']}</p>
            </div>
            """, unsafe_allow_html=True)
        
        # ì‘ë‹µ ë‚´ìš©
        st.markdown("### ğŸ’¬ AI ì‘ë‹µ")
        if result.get('error', False):
            st.error(result['response'])
        else:
            # ì‘ë‹µì„ ë” ë³´ê¸° ì¢‹ê²Œ í‘œì‹œ
            st.text_area(
                "ì‘ë‹µ ë‚´ìš©",
                value=result['response'],
                height=400,
                disabled=True,
                key="main_response"
            )
        
        # ë©”íŠ¸ë¦­ í‘œì‹œ
        col_metric1, col_metric2, col_metric3 = st.columns(3)
        
        with col_metric1:
            st.metric(
                "í† í° ì‚¬ìš©ëŸ‰",
                result.get('tokens_used', 0),
                delta=None
            )
        
        with col_metric2:
            st.metric(
                "ì‘ë‹µ ê¸¸ì´",
                len(result['response']),
                delta=None
            )
        
        with col_metric3:
            cost = result.get('cost', 0.0)
            st.metric(
                "ì˜ˆìƒ ë¹„ìš©",
                f"${cost:.6f}" if cost > 0 else "$0.000000",
                delta=None
            )

# í•˜ë‹¨ ì„¹ì…˜ - ëˆ„ì  ê¸°ë¡
st.markdown("---")
st.header("ğŸ“š ëˆ„ì  ê¸°ë¡")

if st.session_state.results_history:
    # ê¸°ë¡ í•„í„°ë§ ì˜µì…˜
    col_filter1, col_filter2, col_filter3, col_filter4 = st.columns(4)
    
    with col_filter1:
        filter_model = st.selectbox(
            "ëª¨ë¸ í•„í„°",
            ["ì „ì²´"] + list(set([r['model'] for r in st.session_state.results_history]))
        )
    
    with col_filter2:
        show_count = st.number_input(
            "í‘œì‹œí•  ê¸°ë¡ ìˆ˜",
            min_value=1,
            max_value=len(st.session_state.results_history),
            value=min(5, len(st.session_state.results_history))
        )
    
    with col_filter3:
        # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        df_export = pd.DataFrame(st.session_state.results_history)
        
        # ë‚´ë³´ë‚¼ ë°ì´í„° ì •ë¦¬
        export_data = []
        for record in st.session_state.results_history:
            export_row = {
                'ì‹œê°„': record.get('timestamp', ''),
                'ëª¨ë¸': record.get('model', ''),
                'í”„ë¡¬í”„íŠ¸': record.get('prompt', ''),
                'ì‘ë‹µ': record.get('response', ''),
                'ì˜¨ë„': record.get('temperature', 0),
                'í† í°_ì‚¬ìš©ëŸ‰': record.get('tokens_used', 0),
                'ì…ë ¥_í† í°': record.get('input_tokens', 0),
                'ì¶œë ¥_í† í°': record.get('output_tokens', 0),
                'ì˜ˆìƒ_ë¹„ìš©_USD': record.get('cost', 0),
                'ìƒíƒœ': 'ì˜¤ë¥˜' if record.get('error', False) else 'ì„±ê³µ',
                'ë°ì´í„°_ë¯¸ë¦¬ë³´ê¸°': record.get('data_preview', '')
            }
            export_data.append(export_row)
        
        df_export = pd.DataFrame(export_data)
        
        # ì—¬ëŸ¬ í™•ì‹¤í•œ ë‹¤ìš´ë¡œë“œ ì˜µì…˜ ì œê³µ
        col_download1, col_download2 = st.columns(2)
        
        with col_download1:
            # 1. JSON ë‹¤ìš´ë¡œë“œ (ê°€ì¥ í™•ì‹¤)
            json_data = df_export.to_json(orient='records', force_ascii=False, indent=2)
            
            st.download_button(
                label="ğŸ“„ JSON ë‹¤ìš´ë¡œë“œ",
                data=json_data,
                file_name=f"llmops_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                help="JSON í˜•ì‹ - ë©”ëª¨ì¥ì—ì„œ ì—´ì–´ì„œ í™•ì¸ ê°€ëŠ¥"
            )
            
            # 2. íŒŒì´í”„ êµ¬ë¶„ TXT (Excel ê°€ì ¸ì˜¤ê¸°ìš©)
            pipe_data = df_export.to_csv(index=False, sep='|', encoding='utf-8')
            
            st.download_button(
                label="ğŸ“Š íŒŒì´í”„ êµ¬ë¶„ TXT",
                data=pipe_data,
                file_name=f"llmops_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}_pipe.txt",
                mime="text/plain",
                help="íŒŒì´í”„(|)ë¡œ êµ¬ë¶„ - Excelì—ì„œ 'ë°ì´í„° ê°€ì ¸ì˜¤ê¸°' ì‚¬ìš©"
            )
        
        with col_download2:
            # 3. HTML ë‹¤ìš´ë¡œë“œ (ë¸Œë¼ìš°ì €ì—ì„œ ë³µì‚¬ ë¶™ì—¬ë„£ê¸°ìš©)
            html_simple = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>LLMOps History</title>
</head>
<body>
    <h1>LLMOps Dashboard ê¸°ë¡</h1>
    <p>ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    {df_export.to_html(index=False, escape=False)}
</body>
</html>"""
            
            st.download_button(
                label="ğŸŒ HTML ë‹¤ìš´ë¡œë“œ",
                data=html_simple,
                file_name=f"llmops_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html",
                mime="text/html",
                help="HTML íŒŒì¼ - ë¸Œë¼ìš°ì €ì—ì„œ ì—´ì–´ì„œ í‘œë¥¼ ë³µì‚¬ ê°€ëŠ¥"
            )
            
            # 4. ì´ë©”ì¼ ì¹œí™”ì  í…ìŠ¤íŠ¸
            email_text = f"""LLMOps Dashboard ê¸°ë¡
ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
{'='*60}

"""
            for i, row in df_export.iterrows():
                email_text += f"[ê¸°ë¡ {i+1}]\n"
                for col, value in row.items():
                    email_text += f"â€¢ {col}: {value}\n"
                email_text += "\n" + "-"*40 + "\n\n"
            
            st.download_button(
                label="ğŸ“§ ì´ë©”ì¼ìš© TXT",
                data=email_text,
                file_name=f"llmops_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}_email.txt",
                mime="text/plain",
                help="ì´ë©”ì¼ ë³¸ë¬¸ì— ë¶™ì—¬ë„£ê¸° ê°€ëŠ¥í•œ í˜•ì‹"
            )
        
        # ì‚¬ìš©ë²• ì•ˆë‚´
        st.markdown("---")
        st.markdown("### ğŸ’¡ **ì¶”ì²œ ì‚¬ìš©ë²•**")
        st.info("""
**1. JSON ë‹¤ìš´ë¡œë“œ** â†’ ë©”ëª¨ì¥ìœ¼ë¡œ ì—´ì–´ì„œ ë‚´ìš© í™•ì¸ âœ…

**2. HTML ë‹¤ìš´ë¡œë“œ** â†’ í¬ë¡¬ì—ì„œ ì—´ì–´ì„œ í‘œë¥¼ ë“œë˜ê·¸ë¡œ ì„ íƒ â†’ ë³µì‚¬ â†’ Excelì— ë¶™ì—¬ë„£ê¸° âœ…

**3. íŒŒì´í”„ êµ¬ë¶„ TXT** â†’ Excelì—ì„œ 'ë°ì´í„°' â†’ 'í…ìŠ¤íŠ¸/CSVì—ì„œ' â†’ êµ¬ë¶„ê¸°í˜¸ë¥¼ íŒŒì´í”„(|)ë¡œ ì„¤ì • âœ…

**4. ì´ë©”ì¼ìš© TXT** â†’ ë©”ëª¨ì¥ì—ì„œ ì—´ì–´ì„œ ì½ê¸° í¸í•œ í˜•ì‹ âœ…
        """)
        
        # ë¸Œë¼ìš°ì €ì—ì„œ ë°”ë¡œ ë³´ê¸° (ë³µì‚¬ìš©)
        with st.expander("ğŸ–¥ï¸ ë¸Œë¼ìš°ì €ì—ì„œ ë°”ë¡œ ë³´ê¸° (ë³µì‚¬ìš©)"):
            st.dataframe(df_export, use_container_width=True)
            st.markdown("**ìœ„ í‘œë¥¼ ë“œë˜ê·¸ë¡œ ì„ íƒ â†’ ë³µì‚¬ â†’ Excelì— ë¶™ì—¬ë„£ê¸°**")
    
    with col_filter4:
        if st.button("ğŸ—‘ï¸ ê¸°ë¡ ì´ˆê¸°í™”"):
            st.session_state.results_history = []
            st.session_state.current_result = None
            st.rerun()
    
    # í•„í„°ë§ëœ ê¸°ë¡ í‘œì‹œ
    filtered_history = st.session_state.results_history
    if filter_model != "ì „ì²´":
        filtered_history = [r for r in filtered_history if r['model'] == filter_model]
    
    # ìµœì‹  ê¸°ë¡ë¶€í„° í‘œì‹œ
    filtered_history = filtered_history[-show_count:][::-1]
    
    # ê¸°ë¡ì„ íƒ­ìœ¼ë¡œ í‘œì‹œ
    if filtered_history:
        for i, record in enumerate(filtered_history):
            status_icon = "âŒ" if record.get('error', False) else "âœ…"
            with st.expander(f"{status_icon} {record['timestamp']} - {record['model']}", expanded=(i==0)):
                col_record1, col_record2 = st.columns([2, 1])
                
                with col_record1:
                    st.markdown("**í”„ë¡¬í”„íŠ¸:**")
                    st.text(record.get('prompt', 'N/A'))
                    
                    st.markdown("**ì‘ë‹µ:**")
                    if record.get('error', False):
                        st.error(record['response'])
                    else:
                        st.text_area(
                            "ì‘ë‹µ ë‚´ìš©",
                            value=record['response'],
                            height=200,
                            key=f"response_{i}",
                            disabled=True
                        )
                
                with col_record2:
                    st.markdown("**ì„¤ì • ì •ë³´**")
                    record_info = {
                        "ëª¨ë¸": record['model'],
                        "ì˜¨ë„": record['temperature'],
                        "í† í°": record.get('tokens_used', 0),
                        "ì‹œê°„": record['timestamp'],
                        "ìƒíƒœ": "ì˜¤ë¥˜" if record.get('error', False) else "ì„±ê³µ"
                    }
                    
                    # ë¹„ìš© ì •ë³´ ì¶”ê°€
                    if record.get('cost', 0) > 0:
                        record_info["ì˜ˆìƒ ë¹„ìš©"] = f"${record.get('cost', 0):.6f}"
                    
                    st.json(record_info)
    
    # í†µê³„ ìš”ì•½
    if len(st.session_state.results_history) > 1:
        st.markdown("### ğŸ“ˆ ì„¸ì…˜ ìš”ì•½")
        
        # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
        df_history = pd.DataFrame(st.session_state.results_history)
        
        col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
        
        with col_stat1:
            st.metric("ì´ ì§ˆì˜", len(df_history))
        
        with col_stat2:
            avg_temp = df_history['temperature'].mean()
            st.metric("í‰ê·  ì˜¨ë„", f"{avg_temp:.2f}")
        
        with col_stat3:
            total_tokens = df_history['tokens_used'].sum()
            st.metric("ì´ í† í°", total_tokens)
        
        with col_stat4:
            success_rate = len([r for r in st.session_state.results_history if not r.get('error', False)]) / len(st.session_state.results_history) * 100
            st.metric("ì„±ê³µë¥ ", f"{success_rate:.1f}%")
        
        # ì´ ë¹„ìš© ê³„ì‚° ë° í‘œì‹œ
        if st.session_state.results_history:
            total_cost = sum([r.get('cost', 0) for r in st.session_state.results_history])
            if total_cost > 0:
                st.markdown("### ğŸ’° ë¹„ìš© ì •ë³´")
                col_cost1, col_cost2, col_cost3 = st.columns(3)
                with col_cost1:
                    st.metric("ì´ ì‚¬ìš© ë¹„ìš©", f"${total_cost:.6f}")
                with col_cost2:
                    avg_cost = total_cost / len(st.session_state.results_history)
                    st.metric("í‰ê·  ì§ˆì˜ ë¹„ìš©", f"${avg_cost:.6f}")
                with col_cost3:
                    # ë¹„ìš© ìš”ì•½ ë‹¤ìš´ë¡œë“œ (ê°„ë‹¨í•œ í…ìŠ¤íŠ¸)
                    cost_summary_text = f"""LLMOps ë¹„ìš© ìš”ì•½
ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
{'='*40}

ì´ ì§ˆì˜ìˆ˜: {len(st.session_state.results_history)}
ì´ ë¹„ìš©: ${total_cost:.6f}
í‰ê·  ì§ˆì˜ ë¹„ìš©: ${avg_cost:.6f}
ì´ í† í°: {sum([r.get('tokens_used', 0) for r in st.session_state.results_history]):,}
ì„±ê³µë¥ : {len([r for r in st.session_state.results_history if not r.get('error', False)]) / len(st.session_state.results_history) * 100:.1f}%
"""
                    
                    st.download_button(
                        label="ğŸ“Š ë¹„ìš© ìš”ì•½ ë‹¤ìš´ë¡œë“œ",
                        data=cost_summary_text,
                        file_name=f"cost_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                        mime="text/plain",
                        help="ë¹„ìš© ìš”ì•½ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤"
                    )

else:
    st.info("ğŸ“‹ ì•„ì§ ì‹¤í–‰ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ ì‹¤í–‰í•´ë³´ì„¸ìš”!")

# í‘¸í„°
st.markdown("---")
st.markdown(
    "<div style='text-align: center; color: gray;'>"
    "ğŸ¤– Prompt LLMOps Dashboard v2.0 | "
    "Built with Streamlit | Real API Integration"
    "</div>",
    unsafe_allow_html=True
)